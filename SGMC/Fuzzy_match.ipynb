{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to Install\n",
    "\n",
    "<ol>\n",
    "  <li>pip install  fuzzywuzzy</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Load the first CSV file into a Pandas dataframe\n",
    "df1 = pd.read_csv(\"./Data/sample_data_Scrape_Result.csv\")\n",
    "# Load the second CSV file into a second Pandas dataframe\n",
    "df2 = pd.read_csv(\"./Data/Globe-institution_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold the results\n",
    "results = []\n",
    "# Set to hold the 'Acc' values for which matches have already been found\n",
    "matched_ids = set()\n",
    "# Iterate over the columns in df1\n",
    "for column in ['Center Names', 'Submitted_by', 'Biosample_Submission']:\n",
    "    # Iterate over the rows in df1\n",
    "    for index, row in tqdm(df1.iterrows(), total=df1.shape[0], desc='Rows', leave=False):\n",
    "        # Check if this 'Acc' has already been matched\n",
    "        if row['Acc'] in matched_ids:\n",
    "            continue\n",
    "        # Convert the series to a string\n",
    "        string = str(row[column])\n",
    "        # Check if the column contains a comma\n",
    "        if ',' in string:\n",
    "            # Split the string into two parts based on the comma\n",
    "            parts = string.split(',')\n",
    "            # Find the closest match in df2 for each part\n",
    "            matches = [process.extractOne(part.strip(), df2['Institution'], scorer=fuzz.token_sort_ratio) for part in parts]\n",
    "            # Check if there is a match for both parts\n",
    "            if all(match and match[1] >= 70 for match in matches):\n",
    "                # Get the indices of the matching rows in df2\n",
    "                match_indices = [df2[df2['Institution'] == match[0]].index[0] for match in matches]\n",
    "                # Concatenate the rows from df1 and df2 and add to the results list\n",
    "                result = pd.concat([df1.loc[[index], :].reset_index(drop=True), df2.loc[match_indices, :].reset_index(drop=True)], axis=1)\n",
    "                result['Score'] = sum(match[1] for match in matches) / len(matches)\n",
    "                results.append(result)\n",
    "                # Add the 'Acc' value to the set of matched ids\n",
    "                matched_ids.add(row['Acc'])\n",
    "        else:\n",
    "            # Find the closest match in df2\n",
    "            match = process.extractOne(string, df2['Institution'], scorer=fuzz.token_sort_ratio)\n",
    "            if match and match[1] >= 70:\n",
    "                # Get the index of the matching row in df2\n",
    "                match_index = df2[df2['Institution'] == match[0]].index[0]\n",
    "                # Concatenate the rows from df1 and df2 and add to the results list\n",
    "                result = pd.concat([df1.loc[[index], :].reset_index(drop=True), df2.loc[[match_index], :].reset_index(drop=True)], axis=1)\n",
    "                result['Score'] = match[1]\n",
    "                results.append(result)\n",
    "                # Add the 'Acc' value to the set of matched ids\n",
    "                matched_ids.add(row['Acc'])\n",
    "            else:\n",
    "                # If no match is found or the Score is below 70%, add the row from df1 to the results list\n",
    "                result = df1.loc[[index], :].reset_index(drop=True)\n",
    "                result['Score'] = None\n",
    "                results.append(result)\n",
    "# Concatenate all the results into a single dataframe\n",
    "final_result = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.head(45)\n",
    "# final_result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the results into separate dataframes based on the Score score\n",
    "above_90 = final_result[final_result['Score'] >= 90]\n",
    "above_80 = final_result[(final_result['Score'] >= 80) & (final_result['Score'] < 90)]\n",
    "above_70 = final_result[(final_result['Score'] >= 70) & (final_result['Score'] < 80)]\n",
    "below_70 = final_result[final_result['Score'] < 70]\n",
    "below_69 = below_70[below_70['Score'] < 69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the number of unique 'Acc' values in final_result matches the number of rows in df1\n",
    "if final_result['Acc'].nunique() == df1.shape[0]:\n",
    "    print(\"Result contains a row for every 'Acc' in df1\")\n",
    "else:\n",
    "    print(\"Result does not contain a row for every 'Acc' in df1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "above_90.count(),above_90_1.count(),above_90_2.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write each dataframe to a separate CSV file\n",
    "#Save each dataframe to a separate CSV file\n",
    "above_90.to_csv('./Data_Result/sample_data_above_90.csv', index=False, encoding='utf-8')\n",
    "above_80.to_csv('./Data_Result/sample_data_above_80.csv', index=False, encoding='utf-8')\n",
    "above_70.to_csv('./Data_Result/sample_data_above_70.csv', index=False, encoding='utf-8')\n",
    "below_69.to_csv('./Data_Result/sample_data_below_69.csv', index=False, encoding='utf-8')\n",
    "final_result.to_csv('./Data_Result/sample_data_Fuzzy_result.csv', encoding='utf-8')\n",
    "final_result.to_csv('./Data/sample_data_Fuzzy_result.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
